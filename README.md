# Distributed Serving with Multi-GPU LLMs in Kubernetes / OpenShift

This repository provides instructions for deploying LLMs with Multi-GPUs in distributed OpenShift / Kubernetes nodes.

## Usage

TBD

## Links of Interest

* [Distributed Serving Documentation](https://docs.vllm.ai/en/latest/serving/distributed_serving.html)
* [vLLM Engine Args](https://docs.vllm.ai/en/latest/models/engine_args.html)
* [Tensor Parallelism HF Transformers](https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism)